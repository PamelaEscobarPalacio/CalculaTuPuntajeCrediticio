# -*- coding: utf-8 -*-
"""Trabajo_Parte2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ebtFjzPWcV95F-bQ-CmUxQ0iZ4-zOqa
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
import seaborn as sns
import sklearn
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
import numpy as np

from scipy.stats import chi2_contingency
from sklearn.feature_selection import f_classif
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc

datos=pd.read_csv("//content/credit_risk_dataset.csv",delimiter=',')
datos

datos.describe().round(1)

datos.dtypes
print('\nlos datos son de tipo:\n',datos.dtypes)

"""# Identify the target variable"""

# explore the unique values in loan_status column
datos['loan_status'].value_counts(normalize = True)

"""# Split Data"""

# split data into 80/20 while keeping the distribution of bad loans in test set same as that in the pre-split dataset
X = datos.drop('loan_status', axis = 1)
y = datos['loan_status']
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)

# specifically hard copying the training sets to avoid Pandas' SetttingWithCopyWarning when we play around with this data later on.
# as noted [here](https://github.com/scikit-learn/scikit-learn/issues/8723), this is currently an open issue between Pandas and Scikit-Learn teams
x_train, x_test = x_train.copy(), x_test.copy()

x_train

"""# Feature Selection"""

# first divide training data into categorical and numerical subsets
X_train_cat = x_train.select_dtypes(include = 'object').copy()
X_train_num = x_train.select_dtypes(include = 'number').copy()

"""# Chi-squared statistic for categorical features"""

# define an empty dictionary to store chi-squared test results
chi2_check = {}

# loop over each column in the training set to calculate chi-statistic with the target variable
for column in X_train_cat:
    chi, p, dof, ex = chi2_contingency(pd.crosstab(y_train, X_train_cat[column]))
    chi2_check.setdefault('Feature',[]).append(column)
    chi2_check.setdefault('p-value',[]).append(round(p, 10))

# convert the dictionary to a DF
chi2_result = pd.DataFrame(data = chi2_check)
chi2_result.sort_values(by = ['p-value'], ascending = True, ignore_index = True, inplace = True)
chi2_result

"""# Definir ANOVA"""

# since f_class_if does not accept missing values, we will do a very crude imputation of missing values
X_train_num.fillna(X_train_num.mean(), inplace = True)
# Calculate F Statistic and corresponding p values
F_statistic, p_values = f_classif(X_train_num, y_train)
# convert to a DF
ANOVA_F_table = pd.DataFrame(data = {'Numerical_Feature': X_train_num.columns.values, 'F-Score': F_statistic, 'p values': p_values.round(decimals=10)})
ANOVA_F_table.sort_values(by = ['F-Score'], ascending = False, ignore_index = True, inplace = True)
ANOVA_F_table

"""# Pair wise correlations to detect multicollinearity"""

# save the top 20 numerical features in a list
top_num_features = ANOVA_F_table.iloc[:33,0].to_list()
# calculate pair-wise correlations between them
corrmat = X_train_num[top_num_features].corr()
mask = np.triu(np.ones_like(corrmat, dtype=np.bool))
f, ax = plt.subplots(figsize=(20,20))
cmap = sns.diverging_palette(220, 10, as_cmap=True)

sns.heatmap(corrmat, cmap= cmap, mask= mask, vmax=1, center=0.5,
            square=True, linewidths=.5, cbar_kws={"shrink": .6},annot=True)

plt.title("Pearson correlation", fontsize =10)

"""# Creating Dummy Variables

# Convert discrete variables to dummy variables
"""

x_train.info()

x_train

# function to create dummy variables
def dummy_creation(df, columns_list):
    df_dummies = []
    for col in columns_list:
        df_dummies.append(pd.get_dummies(df[col], prefix = col, prefix_sep = ':'))
    df_dummies = pd.concat(df_dummies, axis = 1)
    df = pd.concat([df, df_dummies], axis = 1)
    return df

# apply to our final four categorical variables
x_train = dummy_creation(x_train, ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'])

x_train

"""# Update the test data set with all data cleaning procedures performed so far"""

x_test

x_test = dummy_creation(x_test, ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'])
# reindex the dummied test set variables to make sure all the feature columns in the train set are also available in the test set
x_test = x_test.reindex(labels=x_train.columns, axis=1, fill_value=0)

x_test

"""# WoE Binning/Feature Engineering"""

# Create copies of the 4 training sets to be preprocessed using WoE
X_train_prepr = x_train.copy()
y_train_prepr = y_train.copy()
X_test_prepr = x_test.copy()
y_test_prepr = y_test.copy()



"""# Analyze WoEs and IVs of discrete features"""

# The function takes 3 arguments: a dataframe (X_train_prepr), a string (column name), and a dataframe (y_train_prepr).
# The function returns a dataframe as a result.
def woe_discrete(df, cat_variabe_name, y_df):
    df = pd.concat([df[cat_variabe_name], y_df], axis = 1)
    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),
                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)
    df = df.iloc[:, [0, 1, 3]]
    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']
    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()
    df['n_good'] = df['prop_good'] * df['n_obs']
    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']
    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()
    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()
    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])
    df = df.sort_values(['WoE'])
    df = df.reset_index(drop = True)
    df['diff_prop_good'] = df['prop_good'].diff().abs()
    df['diff_WoE'] = df['WoE'].diff().abs()
    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
    df['IV'] = df['IV'].sum()
    return df

# We set the default style of the graphs to the seaborn style.
sns.set()
# Below we define a function for plotting WoE across categories that takes 2 arguments: a dataframe and a number.
def plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):
    x = np.array(df_WoE.iloc[:, 0].apply(str))
    y = df_WoE['WoE']
    plt.figure(figsize=(18, 6))
    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')
    plt.xlabel(df_WoE.columns[0])
    plt.ylabel('Weight of Evidence')
    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))
    plt.xticks(rotation = rotation_of_x_axis_labels)

media_person_age =datos[datos['person_age'] <= 70]['person_age'].mean()

datos.loc[datos['person_age'] > 70, 'person_age'] = media_person_age
print(datos)

datos.describe().round(1)

datos=pd.DataFrame(datos)
datos.isnull()

datos.isnull().sum()

datos=datos.dropna()

datos.isnull().sum()

datos.duplicated().sum()

datos=datos.drop_duplicates()
datos.duplicated().sum()

df_temp = woe_discrete(X_train_prepr, 'person_home_ownership', y_train_prepr)
df_temp

df_temp = woe_discrete(X_train_prepr, 'loan_intent', y_train_prepr)
df_temp

df_temp = woe_discrete(X_train_prepr, 'loan_grade', y_train_prepr)
df_temp

df_temp = woe_discrete(X_train_prepr, 'cb_person_default_on_file', y_train_prepr)
df_temp

"""# Analyze WoEs and IVs of numeric features"""

# We define a function to calculate WoE of continuous variables. This is same as the function we defined earlier for discrete variables.
# The only difference are the 2 commented lines of code in the function that results in the df being sorted by continuous variable values
def woe_ordered_continuous(df, continuous_variabe_name, y_df):
    df = pd.concat([df[continuous_variabe_name], y_df], axis = 1)
    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),
                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)
    df = df.iloc[:, [0, 1, 3]]
    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']
    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()
    df['n_good'] = df['prop_good'] * df['n_obs']
    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']
    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()
    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()
    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])
    #df = df.sort_values(['WoE'])
    #df = df.reset_index(drop = True)
    df['diff_prop_good'] = df['prop_good'].diff().abs()
    df['diff_WoE'] = df['WoE'].diff().abs()
    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
    df['IV'] = df['IV'].sum()
    return df

df_temp = woe_ordered_continuous(X_train_prepr, 'person_age', y_train_prepr)
df_temp

df_temp = woe_ordered_continuous(X_train_prepr, 'person_income', y_train_prepr)
df_temp

df_temp = woe_ordered_continuous(X_train_prepr, 'person_emp_length', y_train_prepr)
df_temp

df_temp = woe_ordered_continuous(X_train_prepr, 'loan_amnt', y_train_prepr)
df_temp

df_temp = woe_ordered_continuous(X_train_prepr, 'loan_int_rate', y_train_prepr)
df_temp

df_temp = woe_ordered_continuous(X_train_prepr, 'loan_percent_income', y_train_prepr)
df_temp

df_temp = woe_ordered_continuous(X_train_prepr, 'cb_person_cred_hist_length', y_train_prepr)
df_temp

datos.describe().round(1)

"""# person_age_factor"""

#X_train_prepr_temp=X_train_prepr[X_train_prepr['person_age'] <= 70]['person_age'].mean()
X_train_prepr.loc[X_train_prepr['person_age'] > 70, 'person_age'] = media_person_age

# fine-classing using the 'cut' method, given the large number of unique values
X_train_prepr['person_age_factor'] = pd.cut(X_train_prepr['person_age'],
                                            bins=[20,30,40,50,60,70],
                                            labels=['20-30', '30-40', '40-50', '50-60','60-70'])
# Process 'int_rate_factor' column through woe_ordered_continuous and plot_by_woe functions
df_temp = woe_ordered_continuous(X_train_prepr, 'person_age_factor', y_train_prepr)
df_temp

"""# person_income_factor"""

# fine-classing using the 'cut' method, given the large number of unique values
X_train_prepr['person_income_factor'] = pd.cut(X_train_prepr['person_income'],
                                               bins=[4000.0,39500.0, 56000.0, 80000.0, 6000000.0],
                                               labels=['low', 'middle', 'high-middle', 'high'])
# Process 'int_rate_factor' column through woe_ordered_continuous and plot_by_woe functions
df_temp = woe_ordered_continuous(X_train_prepr, 'person_income_factor', y_train_prepr)
df_temp

"""# person_emp_length_factor"""

#X_train_prepr_temp=X_train_prepr[X_train_prepr['person_age'] <= 70]['person_age'].mean()
X_train_prepr.loc[X_train_prepr['person_emp_length'] > 70, 'person_emp_length'] = media_person_age

# fine-classing using the 'cut' method, given the large number of unique values
X_train_prepr['person_emp_length_factor'] = pd.cut(X_train_prepr['person_emp_length'],
                                                   bins=[0, 2, 4, 6,8,41],
                                                   labels=['emp_length_A', 'emp_length_B', 'emp_length_C', 'emp_length_D','emp_length_E'])
# Process 'int_rate_factor' column through woe_ordered_continuous and plot_by_woe functions
df_temp = woe_ordered_continuous(X_train_prepr, 'person_emp_length_factor', y_train_prepr)

"""# loan_amnt_factor"""

# fine-classing using the 'cut' method, given the large number of unique values
X_train_prepr['loan_amnt_factor'] = pd.cut(X_train_prepr['loan_amnt'],
                                           bins=[500.0, 5000.0, 8000.0, 12500.0, 35000.0],
                                           labels=['loan_small', 'loan_medium', 'loan_large', 'loan_very_large'])
# Process 'int_rate_factor' column through woe_ordered_continuous and plot_by_woe functions
df_temp = woe_ordered_continuous(X_train_prepr, 'loan_amnt_factor', y_train_prepr)

"""# loan_int_rate_factor"""

# fine-classing using the 'cut' method, given the large number of unique values
X_train_prepr['loan_int_rate_factor'] = pd.cut(X_train_prepr['loan_int_rate'],
                                               bins=[5.4, 7.9, 11.0, 13.5	,23.2],
                                               labels=['loan_int_A', 'loan_int_B', 'loan_int_C', 'loan_int_D'])
# Process 'int_rate_factor' column through woe_ordered_continuous and plot_by_woe functions
df_temp = woe_ordered_continuous(X_train_prepr, 'loan_int_rate_factor', y_train_prepr)

"""# loan_percent_income_factor"""

# fine-classing using the 'cut' method, given the large number of unique values
X_train_prepr['loan_percent_income_factor'] = pd.cut(X_train_prepr['loan_percent_income'],
                                                     bins=[0.0, 0.1, 0.2, 0.8],
                                                     labels=['A', 'B', 'C'])
# Process 'int_rate_factor' column through woe_ordered_continuous and plot_by_woe functions
df_temp = woe_ordered_continuous(X_train_prepr, 'loan_percent_income_factor', y_train_prepr)

"""# cb_person_cred_hist_length_factor"""

# fine-classing using the 'cut' method, given the large number of unique values
X_train_prepr['cb_person_cred_hist_length_factor'] = pd.cut(X_train_prepr['cb_person_cred_hist_length'],
                                                            bins=[2.0, 3.0, 4.0, 8.0,30.0],
                                                            labels=['hist_length_low', 'hist_length_middle', 'hist_length_high-middle', 'hist_length_high'])
# Process 'int_rate_factor' column through woe_ordered_continuous and plot_by_woe functions
df_temp = woe_ordered_continuous(X_train_prepr, 'cb_person_cred_hist_length_factor', y_train_prepr)

datos

"""# Define Custom Class for WoE Binning/Reengineering

"""

x_train

x_train.info()

X

ref_categories = ['loan_grade:A','person_home_ownership:MORTGAGE','loan_intent:DEBTCONSOLIDATION','cb_person_default_on_file:N','person_age:20-30','person_income:low',
                  'loan_amnt:loan_small','loan_int_rate:loan_int_A','loan_percent_income:A','cb_person_cred_hist_length:hist_length_low']
print(len(ref_categories))

# This custom class will create new categorical dummy features based on the cut-off points that we manually identified
# based on the WoE plots and IV above.
# Given the way it is structured, this class also allows a fit_transform method to be implemented on it, thereby allowing
# us to use it as part of a scikit-learn Pipeline
class WoE_Binning(BaseEstimator, TransformerMixin):
    def __init__(self, X): # no *args or *kargs
        self.X = X
    def fit(self, X, y = None):
        return self #nothing else to do
    def transform(self, X):
        X_new = X.loc[:, 'loan_grade:A': 'loan_grade:G']
        X_new[['person_home_ownership:MORTGAGE','person_home_ownership:OTHER',
               'person_home_ownership:OWN','person_home_ownership:RENT']] = X.loc[:,'person_home_ownership:MORTGAGE':'person_home_ownership:RENT']
        X_new[['loan_intent:DEBTCONSOLIDATION','loan_intent:EDUCATION','loan_intent:HOMEIMPROVEMENT','loan_intent:MEDICAL','loan_intent:PERSONAL','loan_intent:VENTURE']]=X.loc[:,'loan_intent:DEBTCONSOLIDATION':'loan_intent:VENTURE']
        X_new[['cb_person_default_on_file:N','cb_person_default_on_file:Y']]=X.loc[:,'cb_person_default_on_file:N':'cb_person_default_on_file:Y']
        X_new['person_age:20-30'] = np.where((X['person_age'] >= 20) & (X['person_age'] <= 30), 1, 0)
        X_new['person_age:30-40'] = np.where((X['person_age'] > 30) & (X['person_age'] <= 40), 1, 0)
        X_new['person_age:40-50'] = np.where((X['person_age'] > 40) & (X['person_age'] <= 50), 1, 0)
        X_new['person_age:50-60'] = np.where((X['person_age'] > 50) & (X['person_age'] <= 60), 1, 0)

        X_new['person_age:60-70'] = np.where((X['person_age'] > 60) & (X['person_age'] <= 70), 1, 0)
        X_new['person_income:low'] = np.where((X['person_income'] >= 4000.0) & (X['person_income'] <=39500.0), 1, 0)
        X_new['person_income:middle'] = np.where((X['person_income'] > 39500.0) & (X['person_income'] <=56000.0), 1, 0)
        X_new['person_income:high-middle'] = np.where((X['person_income'] >56000.0) & (X['person_income'] <=80000.0), 1, 0)
        X_new['person_income:high'] = np.where((X['person_income'] > 80000.0) & (X['person_income'] <= 6000000.0), 1, 0)
        X_new['loan_amnt:loan_small'] = np.where((X['loan_amnt'] >=500.0) & (X['loan_amnt'] <=5000.0), 1, 0)
        X_new['loan_amnt:loan_medium'] = np.where((X['loan_amnt'] >5000.0) & (X['loan_amnt'] <=8000.0), 1, 0)
        X_new['loan_amnt:loan_large'] = np.where((X['loan_amnt'] > 8000.0) & (X['loan_amnt'] <=12500.0), 1, 0)
        X_new['loan_amnt:loan_very_large'] = np.where((X['loan_amnt'] >12500.0) & (X['loan_amnt'] <=35000.0), 1, 0)
        X_new['loan_int_rate:loan_int_A'] = np.where((X['loan_int_rate'] >= 5.4) & (X['loan_int_rate'] <=7.9), 1, 0)
        X_new['loan_int_rate:loan_int_B'] = np.where((X['loan_int_rate'] >7.9) & (X['loan_int_rate'] <=11.0), 1, 0)
        X_new['loan_int_rate:loan_int_C'] = np.where((X['loan_int_rate'] > 11.0) & (X['loan_int_rate'] <=13.5), 1, 0)
        X_new['loan_int_rate:loan_int_D'] = np.where((X['loan_int_rate'] > 13.5) & (X['loan_int_rate'] <= 23.2), 1, 0)
        X_new['loan_percent_income:A'] = np.where((X['loan_percent_income'] >= 0) & (X['loan_percent_income'] <= 0.1), 1, 0)
        X_new['loan_percent_income:B'] = np.where((X['loan_percent_income'] > 0.1) & (X['loan_percent_income'] <= 0.2), 1, 0)
        X_new['loan_percent_income:C'] = np.where((X['loan_percent_income'] > 0.2) & (X['loan_percent_income'] <= 0.8), 1, 0)
        X_new['cb_person_cred_hist_length:hist_length_low'] = np.where((X['cb_person_cred_hist_length'] >=2.0) & (X['cb_person_cred_hist_length'] <= 3.0), 1, 0)
        X_new['cb_person_cred_hist_length:hist_length_middle'] = np.where((X['cb_person_cred_hist_length'] >3.0) & (X['cb_person_cred_hist_length'] <= 4.0), 1, 0)
        X_new['cb_person_cred_hist_length:hist_length_high-middle'] = np.where((X['loan_percent_income'] > 4.0) & (X['loan_percent_income'] <= 8.0), 1, 0)
        X_new['cb_person_cred_hist_length:hist_length_high'] = np.where((X['loan_percent_income'] > 8.0) & (X['loan_percent_income'] <= 30.0), 1, 0)

        X_new.drop(columns = ref_categories, inplace = True)
        return X_new
# we could have also structured this class without the last drop statement and without creating categories out of the
# feature categories. But doing the way we have done here allows us to keep a proper track of the categories, if required

# reconfirm shape of the 4 datasets
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

x_train

# define modeling pipeline
reg = LogisticRegression(max_iter=1000, class_weight = 'balanced')
woe_transform = WoE_Binning(X)
pipeline = Pipeline(steps=[('woe', woe_transform), ('model', reg)])

# define cross-validation criteria. RepeatedStratifiedKFold automatially takes care of the class imbalance while splitting
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)

# fit and evaluate the logistic regression pipeline with cross-validation as defined in cv
scores = cross_val_score(pipeline, x_train, y_train, scoring = 'roc_auc', cv = cv)
#AUROC = np.mean(scores)
#GINI = AUROC * 2 - 1

# print the mean AUROC score and Gini
#print('Mean AUROC: %.4f' % (AUROC))
#print('Gini: %.4f' % (GINI))

pipeline.fit(x_train, y_train)

#first create a transformed training set through our WoE_Binning custom class
X_train_woe_transformed = woe_transform.fit_transform(x_train)
# Store the column names in X_train as a list
feature_name = X_train_woe_transformed.columns.values
# Create a summary table of our logistic regression model
summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)
# Create a new column in the dataframe, called 'Coefficients', with row values the transposed coefficients from the 'LogisticRegression' model
summary_table['Coefficients'] = np.transpose(pipeline['model'].coef_)
# Increase the index of every row of the dataframe with 1 to store our model intercept in 1st row
summary_table.index = summary_table.index + 1
# Assign our model intercept to this new row
summary_table.loc[0] = ['Intercept', pipeline['model'].intercept_[0]]
# Sort the dataframe by index
summary_table.sort_index(inplace = True)
summary_table

"""# Prediction"""

# make preditions on our test set
y_hat_test = pipeline.predict(x_test)
# get the predicted probabilities
y_hat_test_proba = pipeline.predict_proba(x_test)
# select the probabilities of only the positive class (class 1 - default)
y_hat_test_proba = y_hat_test_proba[:][: , 1]

# we will now create a new DF with actual classes and the predicted probabilities
# create a temp y_test DF to reset its index to allow proper concaternation with y_hat_test_proba
y_test_temp = y_test.copy()
y_test_temp.reset_index(drop = True, inplace = True)
y_test_proba = pd.concat([y_test_temp, pd.DataFrame(y_hat_test_proba)], axis = 1)
# check the shape to make sure the number of rows is same as that in y_test
y_test_proba.shape

# Rename the columns
y_test_proba.columns = ['y_test_class_actual', 'y_hat_test_proba']
# Makes the index of one dataframe equal to the index of another dataframe.
y_test_proba.index = x_test.index
y_test_proba

"""# Confusion Matrix & AUROC on test set"""

# assign a threshold value to differentiate good with bad
tr = 0.5
# crate a new column for the predicted class based on predicted probabilities and threshold
# We will determine this optimat threshold later in this project
y_test_proba['y_test_class_predicted'] = np.where(y_test_proba['y_hat_test_proba'] > tr, 1, 0)
# create the confusion matrix
confusion_matrix(y_test_proba['y_test_class_actual'], y_test_proba['y_test_class_predicted'], normalize = 'all')

# get the values required to plot a ROC curve
fpr, tpr, thresholds = roc_curve(y_test_proba['y_test_class_actual'], y_test_proba['y_hat_test_proba'])
# plot the ROC curve
plt.plot(fpr, tpr)
# plot a secondary diagonal line, with dashed line style and black color to represent a no-skill classifier
plt.plot(fpr, fpr, linestyle = '--', color = 'k')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve');

# Calculate the Area Under the Receiver Operating Characteristic Curve (AUROC) on our test set
AUROC = roc_auc_score(y_test_proba['y_test_class_actual'], y_test_proba['y_hat_test_proba'])
AUROC

# calculate Gini from AUROC
Gini = AUROC * 2 - 1
Gini

no_skill = len(y_test[y_test == 1]) / len(y)
# plot the no skill precision-recall curve
plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')

# calculate inputs for the PR curve
precision, recall, thresholds = precision_recall_curve(y_test_proba['y_test_class_actual'], y_test_proba['y_hat_test_proba'])
# plot PR curve
plt.plot(recall, precision, marker='.', label='Logistic')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.title('PR curve');

# calculate PR AUC
auc_pr = auc(recall, precision)
auc_pr

summary_table

df_ref_categories = pd.DataFrame(ref_categories, columns = ['Feature name'])
# We create a second column, called 'Coefficients', which contains only 0 values.
df_ref_categories['Coefficients'] = 0
df_ref_categories

# Concatenates two dataframes.
df_scorecard = pd.concat([summary_table, df_ref_categories])
# We reset the index of a dataframe.
df_scorecard.reset_index(inplace = True)
df_scorecard

# create a new column, called 'Original feature name', which contains the value of the 'Feature name' column, up to the column symbol.
df_scorecard['Original feature name'] = df_scorecard['Feature name'].str.split(':').str[0]
df_scorecard

# Define the min and max threshholds for our scorecard
min_score = 300
max_score = 850

# calculate the sum of the minimum coefficients of each category within the original feature name
min_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].min().sum()
# calculate the sum of the maximum coefficients of each category within the original feature name
max_sum_coef = df_scorecard.groupby('Original feature name')['Coefficients'].max().sum()
# create a new columns that has the imputed calculated Score based on the multiplication of the coefficient by the ratio of the differences between
# maximum & minimum score and maximum & minimum sum of cefficients.
df_scorecard['Score - Calculation'] = df_scorecard['Coefficients'] * (max_score - min_score) / (max_sum_coef - min_sum_coef)
# update the calculated score of the Intercept (i.e. the default score for each loan)
df_scorecard.loc[0, 'Score - Calculation'] = ((df_scorecard.loc[0,'Coefficients'] - min_sum_coef) / (max_sum_coef - min_sum_coef)) * (max_score - min_score) + min_score
# round the values of the 'Score - Calculation' column and store them in a new column
df_scorecard['Score - Preliminary'] = df_scorecard['Score - Calculation'].round()
df_scorecard

# check the min and max possible scores of our scorecard
min_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].min().sum()
max_sum_score_prel = df_scorecard.groupby('Original feature name')['Score - Preliminary'].max().sum()
print(min_sum_score_prel)
print(max_sum_score_prel)

# so both our min and max scores are out by +1. we need to manually adjust this
# Which one? We'll evaluate based on the rounding differences of the minimum category within each Original Feature Name.
pd.options.display.max_rows = 102
df_scorecard['Difference'] = df_scorecard['Score - Preliminary'] - df_scorecard['Score - Calculation']
df_scorecard

# look like we can get by deducting 1 from the Intercept
df_scorecard['Score - Final'] = df_scorecard['Score - Preliminary']
df_scorecard.loc[0, 'Score - Final'] = 653
df_scorecard

# Recheck min and max possible scores
print(df_scorecard.groupby('Original feature name')['Score - Final'].min().sum())
print(df_scorecard.groupby('Original feature name')['Score - Final'].max().sum())

"""# Calculating credit scores for all observations in the test data set"""

# first create a transformed test set through our WoE_Binning custom class
X_test_woe_transformed = woe_transform.fit_transform(x_test)
# insert an Intercept column in its beginning to align with the # of rows in scorecard
X_test_woe_transformed.insert(0, 'Intercept', 1)
X_test_woe_transformed.head()

x_test

# get the list of our final scorecard scores
scorecard_scores = df_scorecard['Score - Final']
# check the shapes of test set and scorecard before doing matrix dot multiplication
print(X_test_woe_transformed.shape)
print(scorecard_scores.shape)

# we can see that the test set has 17 less columns than the rows in scorecard due to the reference categories
# since the reference categories will always be scored as 0 based on the scorecard, it is safe to add these categories to the end of test set with 0 values
X_test_woe_transformed = pd.concat([X_test_woe_transformed, pd.DataFrame(dict.fromkeys(ref_categories, [0] * len(X_test_woe_transformed)),
                                                                         index = X_test_woe_transformed.index)], axis = 1)
# Need to reshape scorecard_scores so that it is (102,1) to allow for matrix dot multiplication
scorecard_scores = scorecard_scores.values.reshape(44, 1)
print(X_test_woe_transformed.shape)
print(scorecard_scores.shape)

# matrix dot multiplication of test set with scorecard scores
y_scores = X_test_woe_transformed.dot(scorecard_scores)
y_scores

y_scores.info()

y_scores = y_scores.rename(columns = {0: 'Score'}, inplace = False)

y_scores.sort_values(by='Score')

"""# Setting loan approval cut-offs"""

# Calculate Youden's J-Statistic to identify the best threshhold
J = tpr - fpr
# locate the index of the largest J
ix = np.argmax(J)
best_thresh = thresholds[ix]
print('Best Threshold: %f' % (best_thresh))

# update the threshold value
tr = best_thresh
# crate a new column for the predicted class based on predicted probabilities and threshold
y_test_proba['y_test_class_predicted'] = np.where(y_test_proba['y_hat_test_proba'] > tr, 1, 0)
# create the confusion matrix
confusion_matrix(y_test_proba['y_test_class_actual'], y_test_proba['y_test_class_predicted'], normalize = 'all')

# create a new DF comprising of the thresholds from the ROC output
df_cutoffs = pd.DataFrame(thresholds, columns = ['thresholds'])
# calcue Score corresponding to each threshold
df_cutoffs['Score'] = ((np.log(df_cutoffs['thresholds'] / (1 - df_cutoffs['thresholds'])) - min_sum_coef) *
                       ((max_score - min_score) / (max_sum_coef - min_sum_coef)) + min_score).round()
df_cutoffs

def n_approved(p):
    return np.where(y_test_proba['y_hat_test_proba'] >= p, 1, 0).sum()

df_cutoffs['N Approved'] = df_cutoffs['thresholds'].apply(n_approved)
# Then, we calculate the number of rejected applications for each threshold.
# It is the difference between the total number of applications and the approved applications for that threshold.
df_cutoffs['N Rejected'] = y_test_proba['y_hat_test_proba'].shape[0] - df_cutoffs['N Approved']
# Approval rate equalts the ratio of the approved applications and all applications.
df_cutoffs['Approval Rate'] = df_cutoffs['N Approved'] / y_test_proba['y_hat_test_proba'].shape[0]
# Rejection rate equals one minus approval rate.
df_cutoffs['Rejection Rate'] = 1 - df_cutoffs['Approval Rate']
df_cutoffs

# let's have a look at the approval and rejection rates at our ideal threshold
df_cutoffs[df_cutoffs['thresholds'].between(0.24448, 0.24489)]

"""# Puntaje de cada dato"""

y_scores

x_test

final=x_test.copy()
pamela=y_scores.loc[:,'Score']
pamela

final['Score']=pamela
final

final.to_csv('final_datos.csv', index=False)